{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorsDF = spark.read.format(\"csv\").\\\n",
    "            option(\"header\",\"true\").\\\n",
    "            option(\"inferSchema\",\"true\").\\\n",
    "                csv(\"join-actors.txt\")\n",
    "moviesDF = spark.read.format(\"csv\").\\\n",
    "            option(\"header\",\"true\").\\\n",
    "            option(\"inferSchema\",\"true\").\\\n",
    "            csv(\"join-series.txt\")\n",
    "\n",
    "            \n",
    "moviesDF.join(actorsDF,\"movieid\",\"rightouter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF.createOrReplaceTempView(\"movies\")\n",
    "actorsDF.createOrReplaceTempView(\"actors\")\n",
    "newDF = spark.sql(\"select * from movies m,actors a where a.movieid=m.movieid order by a.id\")\n",
    "newDF.show(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- MSSubClass: integer (nullable = true)\n",
      " |-- LotFrontage: string (nullable = true)\n",
      " |-- LotArea: integer (nullable = true)\n",
      " |-- OverallQual: integer (nullable = true)\n",
      " |-- OverallCond: integer (nullable = true)\n",
      " |-- YearBuilt: integer (nullable = true)\n",
      " |-- YearRemodAdd: integer (nullable = true)\n",
      " |-- MasVnrArea: string (nullable = true)\n",
      " |-- BsmtFinSF1: integer (nullable = true)\n",
      " |-- BsmtFinSF2: integer (nullable = true)\n",
      " |-- BsmtUnfSF: integer (nullable = true)\n",
      " |-- TotalBsmtSF: integer (nullable = true)\n",
      " |-- 1stFlrSF: integer (nullable = true)\n",
      " |-- 2ndFlrSF: integer (nullable = true)\n",
      " |-- LowQualFinSF: integer (nullable = true)\n",
      " |-- GrLivArea: integer (nullable = true)\n",
      " |-- BsmtFullBath: integer (nullable = true)\n",
      " |-- BsmtHalfBath: integer (nullable = true)\n",
      " |-- FullBath: integer (nullable = true)\n",
      " |-- HalfBath: integer (nullable = true)\n",
      " |-- BedroomAbvGr: integer (nullable = true)\n",
      " |-- KitchenAbvGr: integer (nullable = true)\n",
      " |-- TotRmsAbvGrd: integer (nullable = true)\n",
      " |-- Fireplaces: integer (nullable = true)\n",
      " |-- GarageYrBlt: string (nullable = true)\n",
      " |-- GarageCars: integer (nullable = true)\n",
      " |-- GarageArea: integer (nullable = true)\n",
      " |-- WoodDeckSF: integer (nullable = true)\n",
      " |-- OpenPorchSF: integer (nullable = true)\n",
      " |-- EnclosedPorch: integer (nullable = true)\n",
      " |-- 3SsnPorch: integer (nullable = true)\n",
      " |-- ScreenPorch: integer (nullable = true)\n",
      " |-- PoolArea: integer (nullable = true)\n",
      " |-- MiscVal: integer (nullable = true)\n",
      " |-- MoSold: integer (nullable = true)\n",
      " |-- YrSold: integer (nullable = true)\n",
      " |-- SalePrice: integer (nullable = true)\n",
      " |-- MSZoningVec: vector (nullable = true)\n",
      " |-- StreetVec: vector (nullable = true)\n",
      " |-- AlleyVec: vector (nullable = true)\n",
      " |-- LotShapeVec: vector (nullable = true)\n",
      " |-- LandContourVec: vector (nullable = true)\n",
      " |-- UtilitiesVec: vector (nullable = true)\n",
      " |-- LotConfigVec: vector (nullable = true)\n",
      " |-- LandSlopeVec: vector (nullable = true)\n",
      " |-- NeighborhoodVec: vector (nullable = true)\n",
      " |-- Condition1Vec: vector (nullable = true)\n",
      " |-- Condition2Vec: vector (nullable = true)\n",
      " |-- BldgTypeVec: vector (nullable = true)\n",
      " |-- HouseStyleVec: vector (nullable = true)\n",
      " |-- RoofStyleVec: vector (nullable = true)\n",
      " |-- RoofMatlVec: vector (nullable = true)\n",
      " |-- Exterior1stVec: vector (nullable = true)\n",
      " |-- Exterior2ndVec: vector (nullable = true)\n",
      " |-- MasVnrTypeVec: vector (nullable = true)\n",
      " |-- ExterQualVec: vector (nullable = true)\n",
      " |-- ExterCondVec: vector (nullable = true)\n",
      " |-- FoundationVec: vector (nullable = true)\n",
      " |-- BsmtQualVec: vector (nullable = true)\n",
      " |-- BsmtCondVec: vector (nullable = true)\n",
      " |-- BsmtExposureVec: vector (nullable = true)\n",
      " |-- BsmtFinType1Vec: vector (nullable = true)\n",
      " |-- BsmtFinType2Vec: vector (nullable = true)\n",
      " |-- HeatingVec: vector (nullable = true)\n",
      " |-- HeatingQCVec: vector (nullable = true)\n",
      " |-- CentralAirVec: vector (nullable = true)\n",
      " |-- ElectricalVec: vector (nullable = true)\n",
      " |-- KitchenQualVec: vector (nullable = true)\n",
      " |-- FunctionalVec: vector (nullable = true)\n",
      " |-- FireplaceQuVec: vector (nullable = true)\n",
      " |-- GarageTypeVec: vector (nullable = true)\n",
      " |-- GarageFinishVec: vector (nullable = true)\n",
      " |-- GarageQualVec: vector (nullable = true)\n",
      " |-- GarageCondVec: vector (nullable = true)\n",
      " |-- PavedDriveVec: vector (nullable = true)\n",
      " |-- PoolQCVec: vector (nullable = true)\n",
      " |-- FenceVec: vector (nullable = true)\n",
      " |-- MiscFeatureVec: vector (nullable = true)\n",
      " |-- SaleTypeVec: vector (nullable = true)\n",
      " |-- SaleConditionVec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pricesDF = spark.read.format(\"csv\").\\\n",
    "            option(\"header\",\"true\").\\\n",
    "            option(\"inferSchema\",\"true\").\\\n",
    "            csv(\"house-prices-data.csv\")\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "\n",
    "strCols= [\"MSZoning\",\"Street\",\"Alley\",\"LotShape\",\"LandContour\",\n",
    "          \"Utilities\",\"LotConfig\",\"LandSlope\",\"Neighborhood\",\n",
    "          \"Condition1\",\"Condition2\",\"BldgType\",\"HouseStyle\",\"RoofStyle\",\n",
    "         \"RoofMatl\",\"Exterior1st\",\"Exterior2nd\",\"MasVnrType\",\n",
    "         \"ExterQual\",\"ExterCond\",\"Foundation\",\"BsmtQual\",\"BsmtCond\",\n",
    "         \"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"Heating\",\n",
    "         \"HeatingQC\",\"CentralAir\",\"Electrical\",\"KitchenQual\",\"Functional\",\n",
    "         \"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n",
    "         \"PavedDrive\",\"PoolQC\",\"Fence\",\"MiscFeature\",\n",
    "         \"SaleType\",\"SaleCondition\"]\n",
    "for colName in strCols:\n",
    "    indexer = StringIndexer(inputCol=colName, \n",
    "                            outputCol=colName+\"Index\")\n",
    "    pricesDF = indexer.fit(pricesDF).transform(pricesDF)\n",
    "    encoder = OneHotEncoder(inputCol=colName+\"Index\",\n",
    "                            outputCol=colName+\"Vec\")\n",
    "    pricesDF = encoder.transform(pricesDF)\n",
    "    pricesDF=pricesDF.drop(colName)\n",
    "    pricesDF=pricesDF.drop(colName+\"Index\")\n",
    "pricesDF.printSchema() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def makeNAZero(sVal):\n",
    "    if(sVal==\"NA\"):\n",
    "        return 0\n",
    "    return int(sVal)\n",
    "\n",
    "udfMakeNAZero = udf(makeNAZero,IntegerType())\n",
    "NACols= [\"LotFrontage\",\"MasVnrArea\",\"GarageYrBlt\"]\n",
    "for colName in NACols:\n",
    "    pricesDF = pricesDF.withColumn(colName+\"Int\",\n",
    "                               udfMakeNAZero(colName))\n",
    "    pricesDF=pricesDF.drop(colName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricesDF = pricesDF.withColumn(\"label\",pricesDF[\"SalePrice\"])\n",
    "pricesDF = pricesDF.drop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(262,[0,1,2,3,4,5...|208500|\n",
      "|(262,[0,1,2,3,4,5...|181500|\n",
      "|(262,[0,1,2,3,4,5...|223500|\n",
      "|(262,[0,1,2,3,4,5...|140000|\n",
      "|(262,[0,1,2,3,4,5...|250000|\n",
      "|(262,[0,1,2,3,4,5...|143000|\n",
      "|(262,[0,1,2,3,4,5...|307000|\n",
      "|(262,[0,1,2,3,4,5...|200000|\n",
      "|(262,[0,1,2,3,4,5...|129900|\n",
      "|(262,[0,1,2,3,4,5...|118000|\n",
      "|(262,[0,1,2,3,4,5...|129500|\n",
      "|(262,[0,1,2,3,4,5...|345000|\n",
      "|(262,[0,1,2,3,4,5...|144000|\n",
      "|(262,[0,1,2,3,4,5...|279500|\n",
      "|(262,[0,1,2,3,4,5...|157000|\n",
      "|(262,[0,1,2,3,4,5...|132000|\n",
      "|(262,[0,1,2,3,4,5...|149000|\n",
      "|(262,[0,1,2,3,4,5...| 90000|\n",
      "|(262,[0,1,2,3,4,5...|159000|\n",
      "|(262,[0,1,2,3,4,5...|139000|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler=VectorAssembler(inputCols=pricesDF.columns[0:-1],\n",
    "                                 outputCol=\"features\")\n",
    "pricesDF = vectorAssembler.transform(pricesDF)\n",
    "pricesDF.select(\"features\",\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 43962.7\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\")\n",
    "(pricesTrain,pricesTest) = pricesDF.randomSplit([0.7,0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data (Decision Tree)= 43962.7\n"
     ]
    }
   ],
   "source": [
    "dtModel = dt.fit(pricesTrain)\n",
    "predictionsDT = dtModel.transform(pricesTest)\n",
    "dtEval = RegressionEvaluator(labelCol=\"label\", \n",
    "                             predictionCol=\"prediction\", \n",
    "                             metricName=\"rmse\")\n",
    "rmseDT = dtEval.evaluate(predictionsDT)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data (Decision Tree)= %g\" % rmseDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data (Random Forest)= 30235.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "rfModel = rf.fit(pricesTrain)\n",
    "predictionsRF = rfModel.transform(pricesTest)\n",
    "\n",
    "rmseDT = dtEval.evaluate(predictionsRF)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data (Random Forest)= %g\" % rmseDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data (Linear Regression)= 30021.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\",maxIter=20)\n",
    "lrModel = lr.fit(pricesTrain)\n",
    "predictionsLR = lrModel.transform(pricesTest)\n",
    "rmseDT = dtEval.evaluate(predictionsLR)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data (Linear Regression)= %g\" % rmseDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE(TrainValidationSplit)= 28768.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "cvwithLR = LinearRegression(maxIter=15)\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(cvwithLR.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(cvwithLR.fitIntercept, [False, True])\\\n",
    "    .addGrid(cvwithLR.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "tvscvwithLR = TrainValidationSplit(estimator=cvwithLR,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           trainRatio=0.7)\n",
    "\n",
    "modelcvwithLR = tvscvwithLR.fit(pricesTrain)\n",
    "\n",
    "predictionscvwithLR = modelcvwithLR.transform(pricesTest)\n",
    "rmseDT = dtEval.evaluate(predictionscvwithLR)\n",
    "print(\"RMSE(TrainValidationSplit)= %g\" % rmseDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam):  0.01\n",
      "Best Param (MaxIter):  10\n",
      "Best Param (elasticNetParam):  1.0\n",
      "Best Param (intercept):  321308.43194\n"
     ]
    }
   ],
   "source": [
    "bestModel= modelcvwithLR.bestModel\n",
    "print 'Best Param (regParam): ', bestModel._java_obj.getRegParam()\n",
    "print 'Best Param (MaxIter): ', bestModel._java_obj.getMaxIter()\n",
    "print 'Best Param (elasticNetParam): ', bestModel._java_obj.getElasticNetParam()\n",
    "print 'Best Param (intercept): ', bestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "cvwithLR = LinearRegression(maxIter=10)\n",
    "crossvalWithLR = CrossValidator(estimator=cvwithLR,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)  \n",
    "modelCV = crossvalWithLR.fit(pricesTrain)\n",
    "\n",
    "predictionsCV = modelCV.transform(pricesTest)\n",
    "rmseDT = dtEval.evaluate(predictionsCV)\n",
    "print(\"RMSE(TrainValidationSplit)= %g\" % rmseDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
